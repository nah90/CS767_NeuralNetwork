{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS767_hw2_Horak.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nah90/CS767_NeuralNetwork/blob/main/CS767_hw2_Horak.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZSlp3DAjdYf"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "#you may not use this file except in compliance with the License.\n",
        "#You may obtain a copy of the License at\n",
        "\n",
        "#https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "#Unless required by applicable law or agreed to in writing, software\n",
        "#distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "#WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "#See the License for the specific language governing permissions and\n",
        "#limitations under the License.\n",
        "\n",
        "#Copyright 2019 The TensorFlow Authors. Initial code edited by Eric Braude"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wF5wszaj97Y"
      },
      "source": [
        "#Assignment 2- initial data with reduced sample size\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FP5258xjs-v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e231b383-2046-4dda-d9e0-2aca823afac6"
      },
      "source": [
        "#INTENT: Load the MNIST data and show it, raw, on the monitor\n",
        "\n",
        "#Import libraries \n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "#Constants\n",
        "TRAIN_SIZE=7500 #Set constant for training size to lower starting accuracy - 1/8 of original (in half 3 times)\n",
        "TEST_SIZE=1250 #Set appropriate TEST_SIZE- 1/6 of TRAIN_SIZE\n",
        "\n",
        "mnist=tf.keras.datasets.mnist #One of a handful of data sets known to Keras/TensorFlow\n",
        "#mnist.load_data() produces a pair of inpu/output tensors for training\n",
        "#and one for testing\n",
        "\n",
        "#Reduce \n",
        "(X_train,y_train), (X_test,y_test) = mnist.load_data()\n",
        "X_train = X_train[:TRAIN_SIZE,:,:] #Reduce size of X_train to TRAIN_SIZE 7500\n",
        "X_test = X_test[:TEST_SIZE,:,:] #Reduce size of X_test to TEST_SIZE 1250\n",
        "y_train = y_train[:TRAIN_SIZE] #Reduce size of y_train to TRAIN_SIZE 7500\n",
        "y_test  = y_test[:TEST_SIZE] #Reduce size of y_test to TEST_SIZE 1250\n",
        "\n",
        "X_train,X_test=X_train/255, X_test/255  #Scale down input\n",
        "\n",
        "#print(\"===========y_train===========\")\n",
        "#print(tf.shape(y_train))\n",
        "#print(y_train)\n",
        "#print(\"===========X_train===========\")\n",
        "#print(tf.shape(X_train))\n",
        "#print(\"===========X_train element 0 (28 rows of 28 gray values)===========\")\n",
        "print(X_train[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n",
            "[[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.01176471 0.07058824 0.07058824 0.07058824 0.49411765 0.53333333\n",
            "  0.68627451 0.10196078 0.65098039 1.         0.96862745 0.49803922\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.11764706 0.14117647 0.36862745 0.60392157\n",
            "  0.66666667 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
            "  0.88235294 0.6745098  0.99215686 0.94901961 0.76470588 0.25098039\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.19215686 0.93333333 0.99215686 0.99215686 0.99215686\n",
            "  0.99215686 0.99215686 0.99215686 0.99215686 0.99215686 0.98431373\n",
            "  0.36470588 0.32156863 0.32156863 0.21960784 0.15294118 0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.07058824 0.85882353 0.99215686 0.99215686 0.99215686\n",
            "  0.99215686 0.99215686 0.77647059 0.71372549 0.96862745 0.94509804\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.31372549 0.61176471 0.41960784 0.99215686\n",
            "  0.99215686 0.80392157 0.04313725 0.         0.16862745 0.60392157\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.05490196 0.00392157 0.60392157\n",
            "  0.99215686 0.35294118 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.54509804\n",
            "  0.99215686 0.74509804 0.00784314 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.04313725\n",
            "  0.74509804 0.99215686 0.2745098  0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.1372549  0.94509804 0.88235294 0.62745098 0.42352941 0.00392157\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.31764706 0.94117647 0.99215686 0.99215686 0.46666667\n",
            "  0.09803922 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.17647059 0.72941176 0.99215686 0.99215686\n",
            "  0.58823529 0.10588235 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.0627451  0.36470588 0.98823529\n",
            "  0.99215686 0.73333333 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.97647059\n",
            "  0.99215686 0.97647059 0.25098039 0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.18039216 0.50980392 0.71764706 0.99215686\n",
            "  0.99215686 0.81176471 0.00784314 0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.15294118 0.58039216 0.89803922 0.99215686 0.99215686 0.99215686\n",
            "  0.98039216 0.71372549 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.09411765 0.44705882\n",
            "  0.86666667 0.99215686 0.99215686 0.99215686 0.99215686 0.78823529\n",
            "  0.30588235 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.09019608 0.25882353 0.83529412 0.99215686\n",
            "  0.99215686 0.99215686 0.99215686 0.77647059 0.31764706 0.00784314\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.07058824 0.67058824 0.85882353 0.99215686 0.99215686 0.99215686\n",
            "  0.99215686 0.76470588 0.31372549 0.03529412 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.21568627 0.6745098\n",
            "  0.88627451 0.99215686 0.99215686 0.99215686 0.99215686 0.95686275\n",
            "  0.52156863 0.04313725 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.53333333 0.99215686\n",
            "  0.99215686 0.99215686 0.83137255 0.52941176 0.51764706 0.0627451\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3IKyzTCDNGo"
      },
      "source": [
        "model = tf.keras.models.Sequential([ #Layer format for the neural net\n",
        "  tf.keras.layers.Flatten(input_shape=(28, 28)), #Each pixel (grayscale value) mapped to one of 784 nodes\n",
        "  tf.keras.layers.Dense(128, activation='relu'), #Fully connected to hidden layer with relu\n",
        "  #Dropout layer randomly sets its input units to 0 at 20% rate at each training step\n",
        "  #The other inputs are scaled up by 1/0.8 so sum over all inputs is unchanged\n",
        "  #Illustrative figure: http://laid.delanover.com/wp-content/uploads/2018/02/dropout.png\n",
        "  #(see https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout)\n",
        "  tf.keras.layers.Dropout(0.2),  \n",
        "  tf.keras.layers.Dense(10) #e.g., output #7 expresses degree to which the input is a 7\n",
        "])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeOrNdnkEEcR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8750eadc-f87f-4e25-ebf1-1d999e1fb2a7"
      },
      "source": [
        "print(\"===========first element of X_train (29 rows)===========\")\n",
        "print(X_train[:1]) #For each example, model returns a vector of 'logit' scores, one for each class\n",
        "#A tensor where highest value indicates most likely output\n",
        "\n",
        "predictions = model(X_train[:1]).numpy() #numpy() converts the tensor output\n",
        "print(\"===========untrained output of first training set input===========\")\n",
        "predictions #Print predictions"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===========first element of X_train (29 rows)===========\n",
            "[[[0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.01176471 0.07058824 0.07058824 0.07058824 0.49411765 0.53333333\n",
            "   0.68627451 0.10196078 0.65098039 1.         0.96862745 0.49803922\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.11764706 0.14117647 0.36862745 0.60392157\n",
            "   0.66666667 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
            "   0.88235294 0.6745098  0.99215686 0.94901961 0.76470588 0.25098039\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.         0.19215686 0.93333333 0.99215686 0.99215686 0.99215686\n",
            "   0.99215686 0.99215686 0.99215686 0.99215686 0.99215686 0.98431373\n",
            "   0.36470588 0.32156863 0.32156863 0.21960784 0.15294118 0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.         0.07058824 0.85882353 0.99215686 0.99215686 0.99215686\n",
            "   0.99215686 0.99215686 0.77647059 0.71372549 0.96862745 0.94509804\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.31372549 0.61176471 0.41960784 0.99215686\n",
            "   0.99215686 0.80392157 0.04313725 0.         0.16862745 0.60392157\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.05490196 0.00392157 0.60392157\n",
            "   0.99215686 0.35294118 0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.54509804\n",
            "   0.99215686 0.74509804 0.00784314 0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.04313725\n",
            "   0.74509804 0.99215686 0.2745098  0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.1372549  0.94509804 0.88235294 0.62745098 0.42352941 0.00392157\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.31764706 0.94117647 0.99215686 0.99215686 0.46666667\n",
            "   0.09803922 0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.17647059 0.72941176 0.99215686 0.99215686\n",
            "   0.58823529 0.10588235 0.         0.         0.         0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.0627451  0.36470588 0.98823529\n",
            "   0.99215686 0.73333333 0.         0.         0.         0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.97647059\n",
            "   0.99215686 0.97647059 0.25098039 0.         0.         0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.18039216 0.50980392 0.71764706 0.99215686\n",
            "   0.99215686 0.81176471 0.00784314 0.         0.         0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.15294118 0.58039216 0.89803922 0.99215686 0.99215686 0.99215686\n",
            "   0.98039216 0.71372549 0.         0.         0.         0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.09411765 0.44705882\n",
            "   0.86666667 0.99215686 0.99215686 0.99215686 0.99215686 0.78823529\n",
            "   0.30588235 0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.09019608 0.25882353 0.83529412 0.99215686\n",
            "   0.99215686 0.99215686 0.99215686 0.77647059 0.31764706 0.00784314\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.07058824 0.67058824 0.85882353 0.99215686 0.99215686 0.99215686\n",
            "   0.99215686 0.76470588 0.31372549 0.03529412 0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.21568627 0.6745098\n",
            "   0.88627451 0.99215686 0.99215686 0.99215686 0.99215686 0.95686275\n",
            "   0.52156863 0.04313725 0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.53333333 0.99215686\n",
            "   0.99215686 0.99215686 0.83137255 0.52941176 0.51764706 0.0627451\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.        ]]]\n",
            "===========untrained output of first training set input===========\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.06525381,  0.32089993,  0.62820756,  0.14460514, -0.5281236 ,\n",
              "         0.01137353,  1.0299832 , -0.03285824,  0.17087965, -0.02277037]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWSRnQ0WI5eq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c745f6a-e6a2-4ef4-855e-047f6cf0ac58"
      },
      "source": [
        "tf.nn.softmax(predictions).numpy() #tf.nn.softmax function converts logits to \"probabilities\" for each class"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.07274252, 0.10702688, 0.14553078, 0.08972821, 0.04578957,\n",
              "        0.07853572, 0.21749224, 0.07513765, 0.09211701, 0.07589947]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSkzdv8MD0tT"
      },
      "source": [
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) #Loss equal to negative log probability of the true class\n",
        "#'0' if sure of correct class"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJWqEVrrJ7ZB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3e68fb7-5d35-41cd-ebcf-3fe8e2ad7357"
      },
      "source": [
        "loss_fn(y_train[:1], predictions).numpy() #Initial untrained model should give loss as -tf.math.log(1/10)~~ 2.3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.5442019"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9foNKHzTD2Vo"
      },
      "source": [
        "#Put together the NN with training process, loss, and means of evaluation\n",
        "model.compile(optimizer='adam',\n",
        "              loss=loss_fn,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7suUbJXVLqP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b8ff4bc-6fa7-4683-db74-06cab07d8acc"
      },
      "source": [
        "#Model.fit method adjusts the model parameters to minimize the loss\n",
        "model.fit(X_train, y_train, epochs=5) #Train it with 5 epochs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "235/235 [==============================] - 2s 4ms/step - loss: 0.6541 - accuracy: 0.8109\n",
            "Epoch 2/5\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.3077 - accuracy: 0.9121\n",
            "Epoch 3/5\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2380 - accuracy: 0.9320\n",
            "Epoch 4/5\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.1865 - accuracy: 0.9460\n",
            "Epoch 5/5\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.1563 - accuracy: 0.9552\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd7a1ef09d0>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7dTAzgHDUh7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "101cb0fa-9600-4e41-ae40-a76d3981a45e"
      },
      "source": [
        "#Model.evaluate method checks the models performance on a 'Validation-set' or 'Test-set'\n",
        "model.evaluate(X_test,  y_test, verbose=2) #Accuracy = fraction of correct test pairs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40/40 - 0s - loss: 0.2504 - accuracy: 0.9152 - 248ms/epoch - 6ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.250422865152359, 0.9151999950408936]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sh4G5rdVHrll"
      },
      "source": [
        "#First Code Modification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uA4b2P2UHg4m",
        "outputId": "af8860db-3501-49cd-a1f0-ead1b0803e1e"
      },
      "source": [
        "model1 = tf.keras.models.Sequential([ #Layer format for the neural net\n",
        "  tf.keras.layers.Flatten(input_shape=(28, 28)), #Each pixel (grayscale value) mapped to one of 784 nodes\n",
        "  tf.keras.layers.Dense(units=128, activation='relu'), #Fully connected to hidden layer with 'relu'\n",
        "  tf.keras.layers.Dropout(0.2), #Dropout layer randomly sets its input units to 0 at 20% rate at each training step\n",
        "  tf.keras.layers.Dense(units=64, activation='relu'), #Add hidden layer with size=64 'relu'\n",
        "  tf.keras.layers.Dense(units=28, activation='relu'), #Add hidden layer with size=28 'relu'\n",
        "  tf.keras.layers.Dense(units=10) #Final output layer\n",
        "])\n",
        "\n",
        "predictions1 = model1(X_train[:1]).numpy() #numpy() converts the tensor output\n",
        "print(\"===========untrained output of first training set input===========\") #For each example, model returns a vector of 'logit' scores, one for each class\n",
        "#A tensor where highest value indicates most likely output\n",
        "print(predictions1) #Print predictions\n",
        "\n",
        "tf.nn.softmax(predictions1).numpy() #tf.nn.softmax function converts logits to \"probabilities\" for each class\n",
        "print('')\n",
        "print(tf.nn.softmax(predictions1).numpy()) #Print softmax predictions\n",
        "\n",
        "loss_fn1 = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) #Loss equal to negative log probability of the true class\n",
        "#'0' if sure of correct class\n",
        "\n",
        "print('')\n",
        "print(loss_fn1(y_train[:1], predictions1).numpy()) #Initial untrained model should give loss as -tf.math.log(1/10)~~ 2.3\n",
        "print('')\n",
        "\n",
        "#Put together the NN with training process, loss, and means of evaluation\n",
        "model1.compile(optimizer='adam',\n",
        "              loss=loss_fn1,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#Model.fit method adjusts the model parameters to minimize the loss\n",
        "model1.fit(X_train, y_train, epochs=5) #Train it with 5 epochs\n",
        "print('')\n",
        "\n",
        "#Model.evaluate method checks the models performance on a 'Validation-set' or 'Test-set'\n",
        "model1.evaluate(X_test,  y_test, verbose=2) #Accuracy = fraction of correct test pairs\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===========untrained output of first training set input===========\n",
            "[[-0.23467462  0.00186955 -0.00747118  0.01996927 -0.26848364  0.16531032\n",
            "   0.37239632  0.05639692 -0.38697582  0.22759798]]\n",
            "\n",
            "[[0.07758057 0.09828399 0.09737022 0.1000791  0.07500149 0.11573488\n",
            "  0.14236411 0.10379197 0.06662073 0.12317297]]\n",
            "\n",
            "2.1564534\n",
            "\n",
            "Epoch 1/5\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.7186 - accuracy: 0.7860\n",
            "Epoch 2/5\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2781 - accuracy: 0.9195\n",
            "Epoch 3/5\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2022 - accuracy: 0.9412\n",
            "Epoch 4/5\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.1516 - accuracy: 0.9553\n",
            "Epoch 5/5\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1254 - accuracy: 0.9617\n",
            "\n",
            "40/40 - 0s - loss: 0.2434 - accuracy: 0.9264 - 185ms/epoch - 5ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.2433576136827469, 0.9264000058174133]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKv0LrfVPdaL"
      },
      "source": [
        "#Second Code Modification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOlJ35DZO9VJ",
        "outputId": "aedeb33c-0d8d-4561-cf1a-18d996f15b4e"
      },
      "source": [
        "model2 = tf.keras.models.Sequential([ #Layer format for the neural net\n",
        "  tf.keras.layers.Flatten(input_shape=(28, 28)), #Each pixel (grayscale value) mapped to one of 784 nodes\n",
        "  tf.keras.layers.Dense(units=256, activation='relu'), #Fully connected to hidden layer with 'relu'\n",
        "  tf.keras.layers.Dropout(0.20), #Dropout layer randomly sets its input units to 0 at 20% rate at each training step\n",
        "  tf.keras.layers.LeakyReLU(alpha=0.3), #LeakyReLU layer - negative slope coefficient at 0.3\n",
        "  tf.keras.layers.Dense(units=96, activation='relu'), #Add hidden layer with size=96 'relu'\n",
        "  tf.keras.layers.Dense(units=42, activation='relu'), #Add hidden layer with size=42 'relu'\n",
        "  tf.keras.layers.Dense(units=10) #Final output layer\n",
        "])\n",
        "\n",
        "predictions2 = model2(X_train[:1]).numpy() #numpy() converts the tensor output\n",
        "print(\"===========untrained output of first training set input===========\") #For each example, model returns a vector of 'logit' scores, one for each class\n",
        "#A tensor where highest value indicates most likely output\n",
        "print(predictions2) #Print predictions\n",
        "\n",
        "tf.nn.softmax(predictions2).numpy() #tf.nn.softmax function converts logits to \"probabilities\" for each class\n",
        "print('')\n",
        "print(tf.nn.softmax(predictions2).numpy()) #Print softmax predictions\n",
        "\n",
        "loss_fn2 = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) #Loss equal to negative log probability of the true class\n",
        "#'0' if sure of correct class\n",
        "\n",
        "print('')\n",
        "print(loss_fn2(y_train[:1], predictions2).numpy()) #Initial untrained model should give loss as -tf.math.log(1/10)~~ 2.3\n",
        "print('')\n",
        "\n",
        "#Put together the NN with training process, loss, and means of evaluation\n",
        "model2.compile(optimizer='adam',\n",
        "              loss=loss_fn2,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#Model.fit method adjusts the model parameters to minimize the loss\n",
        "model2.fit(X_train, y_train, epochs=5) #Train it with 5 epochs\n",
        "print('')\n",
        "\n",
        "#Model.evaluate method checks the models performance on a 'Validation-set' or 'Test-set'\n",
        "model2.evaluate(X_test,  y_test, verbose=2) #Accuracy = fraction of correct test pairs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===========untrained output of first training set input===========\n",
            "[[-0.00459114  0.17415036  0.37202567  0.35450232  0.18983492 -0.08045578\n",
            "  -0.05044585 -0.25217775  0.0272236  -0.20357607]]\n",
            "\n",
            "[[0.09246933 0.11056666 0.1347598  0.13241893 0.11231452 0.08571368\n",
            "  0.08832493 0.0721892  0.09545852 0.07578438]]\n",
            "\n",
            "2.4567428\n",
            "\n",
            "Epoch 1/5\n",
            "235/235 [==============================] - 2s 6ms/step - loss: 0.5864 - accuracy: 0.8247\n",
            "Epoch 2/5\n",
            "235/235 [==============================] - 1s 6ms/step - loss: 0.2418 - accuracy: 0.9285\n",
            "Epoch 3/5\n",
            "235/235 [==============================] - 1s 6ms/step - loss: 0.1638 - accuracy: 0.9489\n",
            "Epoch 4/5\n",
            "235/235 [==============================] - 1s 6ms/step - loss: 0.1211 - accuracy: 0.9635\n",
            "Epoch 5/5\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.0949 - accuracy: 0.9705\n",
            "\n",
            "40/40 - 0s - loss: 0.2064 - accuracy: 0.9368 - 262ms/epoch - 7ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.20638659596443176, 0.9368000030517578]"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pT9MlBJcCFDZ"
      },
      "source": [
        "#Third Code Modification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZVfmvqTCCdE",
        "outputId": "5e38f7e5-ac05-4f33-9aed-74f7950abffc"
      },
      "source": [
        "model3 = tf.keras.models.Sequential([ #Layer format for the neural net\n",
        "  tf.keras.layers.Flatten(input_shape=(28, 28)), #Each pixel (grayscale value) mapped to one of 784 nodes\n",
        "  tf.keras.layers.Dense(units=256, activation='relu'), #Fully connected to hidden layer with 'relu'\n",
        "  tf.keras.layers.Dropout(0.20), #Dropout layer randomly sets its input units to 0 at 20% rate at each training step\n",
        "  tf.keras.layers.LeakyReLU(alpha=0.3), #LeakyReLU layer - negative slope coefficient at 0.3\n",
        "  tf.keras.layers.Dropout(0.20), #Dropout layer randomly sets units to 0 at 20% rate at each training step\n",
        "  tf.keras.layers.Dense(units=96, activation='relu'), #Add hidden layer with size=96 'relu'\n",
        "  tf.keras.layers.Dropout(0.20), #Dropout layer randomly sets units to 0 at 20% rate at each training step\n",
        "  tf.keras.layers.Dense(units=42, activation='relu'), #Add hidden layer with size=42 'relu'\n",
        "  tf.keras.layers.Dropout(0.20), #Dropout layer randomly sets units to 0 at 20% rate at each training step\n",
        "  tf.keras.layers.Dense(units=10) #Final output layer\n",
        "])\n",
        "\n",
        "predictions3 = model3(X_train[:1]).numpy() #numpy() converts the tensor output\n",
        "print(\"===========untrained output of first training set input===========\") #For each example, model returns a vector of 'logit' scores, one for each class\n",
        "#A tensor where highest value indicates most likely output\n",
        "print(predictions3) #Print predictions\n",
        "\n",
        "tf.nn.softmax(predictions3).numpy() #tf.nn.softmax function converts logits to \"probabilities\" for each class\n",
        "print('')\n",
        "print(tf.nn.softmax(predictions3).numpy()) #Print softmax predictions\n",
        "\n",
        "loss_fn3 = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) #Loss equal to negative log probability of the true class\n",
        "#'0' if sure of correct class\n",
        "\n",
        "print('')\n",
        "print(loss_fn3(y_train[:1], predictions3).numpy()) #Initial untrained model should give loss as -tf.math.log(1/10)~~ 2.3\n",
        "print('')\n",
        "\n",
        "#Put together the NN with training process, loss, and means of evaluation\n",
        "model3.compile(optimizer='adam',\n",
        "              loss=loss_fn3,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#Model.fit method adjusts the model parameters to minimize the loss\n",
        "model3.fit(X_train, y_train, epochs=5) #Train it with 5 epochs\n",
        "print('')\n",
        "\n",
        "#Model.evaluate method checks the models performance on a 'Validation-set' or 'Test-set'\n",
        "model3.evaluate(X_test,  y_test, verbose=2) #Accuracy = fraction of correct test pairs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===========untrained output of first training set input===========\n",
            "[[-0.12781101  0.07565821  0.29559293  0.04302369  0.465162   -0.30641836\n",
            "  -0.00267497  0.20424484  0.00270183  0.16061102]]\n",
            "\n",
            "[[0.07945491 0.09738371 0.12133966 0.09425694 0.14376251 0.06645881\n",
            "  0.09004647 0.1107467  0.09053192 0.1060183 ]]\n",
            "\n",
            "2.7111728\n",
            "\n",
            "Epoch 1/5\n",
            "235/235 [==============================] - 2s 4ms/step - loss: 0.8920 - accuracy: 0.7160\n",
            "Epoch 2/5\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.3834 - accuracy: 0.8888\n",
            "Epoch 3/5\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2846 - accuracy: 0.9179\n",
            "Epoch 4/5\n",
            "235/235 [==============================] - 1s 6ms/step - loss: 0.2312 - accuracy: 0.9332\n",
            "Epoch 5/5\n",
            "235/235 [==============================] - 2s 6ms/step - loss: 0.1928 - accuracy: 0.9455\n",
            "\n",
            "40/40 - 0s - loss: 0.2306 - accuracy: 0.9296 - 275ms/epoch - 7ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.23061266541481018, 0.9296000003814697]"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22ZMkyFI4D5Z"
      },
      "source": [
        "#Fourth Code Modification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnUwjige4Cg4",
        "outputId": "d59e3669-6b01-45cc-a458-48cb3740c22c"
      },
      "source": [
        "model4 = tf.keras.models.Sequential([ #Layer format for the neural net\n",
        "  tf.keras.layers.Flatten(input_shape=(28, 28)), #Each pixel (grayscale value) mapped to one of 784 nodes\n",
        "  tf.keras.layers.Dense(units=256, activation='relu'), #Fully connected to hidden layer with 'relu'\n",
        "  tf.keras.layers.Dropout(0.20), #Dropout layer randomly sets its input units to 0 at 20% rate at each training step\n",
        "  tf.keras.layers.LeakyReLU(alpha=0.3), #LeakyReLU layer - negative slope coefficient at 0.3\n",
        "  tf.keras.layers.Dropout(0.30), #Dropout layer randomly sets units to 0 at 30% rate at each training step\n",
        "  tf.keras.layers.Dense(units=96, activation='relu'), #Add hidden layer with size=96 'relu'\n",
        "  tf.keras.layers.Dropout(0.30), #Dropout layer randomly sets units to 0 at 30% rate at each training step\n",
        "  tf.keras.layers.Dense(units=42, activation='relu'), #Add hidden layer with size=42 'relu'\n",
        "  tf.keras.layers.Dropout(0.30), #Dropout layer randomly sets units to 0 at 30% rate at each training step\n",
        "  tf.keras.layers.Dense(units=10) #Final output layer\n",
        "])\n",
        "\n",
        "predictions4 = model4(X_train[:1]).numpy() #numpy() converts the tensor output\n",
        "print(\"===========untrained output of first training set input===========\") #For each example, model returns a vector of 'logit' scores, one for each class\n",
        "#A tensor where highest value indicates most likely output\n",
        "print(predictions4) #Print predictions\n",
        "\n",
        "tf.nn.softmax(predictions4).numpy() #tf.nn.softmax function converts logits to \"probabilities\" for each class\n",
        "print('')\n",
        "print(tf.nn.softmax(predictions4).numpy()) #Print softmax predictions\n",
        "\n",
        "loss_fn4 = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) #Loss equal to negative log probability of the true class\n",
        "#'0' if sure of correct class\n",
        "\n",
        "print('')\n",
        "print(loss_fn4(y_train[:1], predictions4).numpy()) #Initial untrained model should give loss as -tf.math.log(1/10)~~ 2.3\n",
        "print('')\n",
        "\n",
        "#Put together the NN with training process, loss, and means of evaluation\n",
        "model4.compile(optimizer='adam',\n",
        "              loss=loss_fn4,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#Model.fit method adjusts the model parameters to minimize the loss\n",
        "model4.fit(X_train, y_train, epochs=40) #Train it with 5 epochs\n",
        "print('')\n",
        "\n",
        "#Model.evaluate method checks the models performance on a 'Validation-set' or 'Test-set'\n",
        "model4.evaluate(X_test,  y_test, verbose=2) #Accuracy = fraction of correct test pairs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===========untrained output of first training set input===========\n",
            "[[-0.20015302  0.27866676 -0.29472956  0.31885427  0.19967213  0.03304237\n",
            "   0.09823121  0.46119103 -0.08594295  0.20798685]]\n",
            "\n",
            "[[0.07210556 0.1163905  0.06559861 0.1211632  0.10755004 0.09104251\n",
            "  0.09717517 0.13969691 0.08082943 0.10844802]]\n",
            "\n",
            "2.3964288\n",
            "\n",
            "Epoch 1/40\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 1.1437 - accuracy: 0.6128\n",
            "Epoch 2/40\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.5112 - accuracy: 0.8515\n",
            "Epoch 3/40\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.3945 - accuracy: 0.8860\n",
            "Epoch 4/40\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.3063 - accuracy: 0.9156\n",
            "Epoch 5/40\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2575 - accuracy: 0.9273\n",
            "Epoch 6/40\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2416 - accuracy: 0.9327\n",
            "Epoch 7/40\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.1959 - accuracy: 0.9473\n",
            "Epoch 8/40\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.1942 - accuracy: 0.9443\n",
            "Epoch 9/40\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1731 - accuracy: 0.9525\n",
            "Epoch 10/40\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1609 - accuracy: 0.9557\n",
            "Epoch 11/40\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.1486 - accuracy: 0.9592\n",
            "Epoch 12/40\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.1232 - accuracy: 0.9657\n",
            "Epoch 13/40\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.1331 - accuracy: 0.9629\n",
            "Epoch 14/40\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.1356 - accuracy: 0.9615\n",
            "Epoch 15/40\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1016 - accuracy: 0.9712\n",
            "Epoch 16/40\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.1051 - accuracy: 0.9677\n",
            "Epoch 17/40\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0961 - accuracy: 0.9724\n",
            "Epoch 18/40\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.1112 - accuracy: 0.9709\n",
            "Epoch 19/40\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0903 - accuracy: 0.9755\n",
            "Epoch 20/40\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0798 - accuracy: 0.9769\n",
            "Epoch 21/40\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0774 - accuracy: 0.9775\n",
            "Epoch 22/40\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0847 - accuracy: 0.9768\n",
            "Epoch 23/40\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0748 - accuracy: 0.9777\n",
            "Epoch 24/40\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0681 - accuracy: 0.9797\n",
            "Epoch 25/40\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0593 - accuracy: 0.9832\n",
            "Epoch 26/40\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0633 - accuracy: 0.9833\n",
            "Epoch 27/40\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0761 - accuracy: 0.9787\n",
            "Epoch 28/40\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0710 - accuracy: 0.9803\n",
            "Epoch 29/40\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0616 - accuracy: 0.9819\n",
            "Epoch 30/40\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0741 - accuracy: 0.9777\n",
            "Epoch 31/40\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0624 - accuracy: 0.9824\n",
            "Epoch 32/40\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0584 - accuracy: 0.9844\n",
            "Epoch 33/40\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.0614 - accuracy: 0.9816\n",
            "Epoch 34/40\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0549 - accuracy: 0.9841\n",
            "Epoch 35/40\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0612 - accuracy: 0.9837\n",
            "Epoch 36/40\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0536 - accuracy: 0.9840\n",
            "Epoch 37/40\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0525 - accuracy: 0.9845\n",
            "Epoch 38/40\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0520 - accuracy: 0.9841\n",
            "Epoch 39/40\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0392 - accuracy: 0.9885\n",
            "Epoch 40/40\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0614 - accuracy: 0.9833\n",
            "\n",
            "40/40 - 0s - loss: 0.2640 - accuracy: 0.9528 - 185ms/epoch - 5ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.26401129364967346, 0.9527999758720398]"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNIUznRTPsMJ"
      },
      "source": [
        "#Appendix A Code Modification\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4CWsKWgP19h",
        "outputId": "1e4df169-5174-46a6-a01f-ac3a45333d1f"
      },
      "source": [
        "modelA = tf.keras.models.Sequential([ #Layer format for the neural net\n",
        "  tf.keras.layers.Flatten(input_shape=(28, 28)), #Each pixel (grayscale value) mapped to one of 784 nodes\n",
        "  tf.keras.layers.Dense(units=128, activation='relu'), #Fully connected to hidden layer with 'relu'\n",
        "  tf.keras.layers.Dropout(0.2), #Dropout layer randomly sets its input units to 0 at 20% rate at each training step\n",
        "  tf.keras.layers.Dense(units=64, activation='sigmoid'), #Add hidden layer with size=64 'sigmoid'\n",
        "  tf.keras.layers.Dense(units=28, activation='sigmoid'), #Add hidden layer with size=32 'sigmoid'\n",
        "  tf.keras.layers.Dense(units=10) #Final output layer\n",
        "])\n",
        "\n",
        "predictionsA = modelA(X_train[:1]).numpy() #numpy() converts the tensor output\n",
        "print(\"===========untrained output of first training set input===========\") #For each example, model returns a vector of 'logit' scores, one for each class\n",
        "#A tensor where highest value indicates most likely output\n",
        "print(predictionsA) #Print predictions\n",
        "\n",
        "tf.nn.softmax(predictionsA).numpy() #tf.nn.softmax function converts logits to \"probabilities\" for each class\n",
        "print('')\n",
        "print(tf.nn.softmax(predictionsA).numpy()) #Print softmax predictions\n",
        "\n",
        "loss_fnA = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) #Loss equal to negative log probability of the true class\n",
        "#'0' if sure of correct class\n",
        "\n",
        "print('')\n",
        "print(loss_fnA(y_train[:1], predictionsA).numpy()) #Initial untrained model should give loss as -tf.math.log(1/10)~~ 2.3\n",
        "print('')\n",
        "\n",
        "#Put together the NN with training process, loss, and means of evaluation\n",
        "modelA.compile(optimizer='adam',\n",
        "              loss=loss_fnA,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#Model.fit method adjusts the model parameters to minimize the loss\n",
        "modelA.fit(X_train, y_train, epochs=5) #Train it with 5 epochs\n",
        "print('')\n",
        "\n",
        "#Model.evaluate method checks the models performance on a 'Validation-set' or 'Test-set'\n",
        "modelA.evaluate(X_test,  y_test, verbose=2) #Accuracy = fraction of correct test pairs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===========untrained output of first training set input===========\n",
            "[[ 1.7327286   0.21384455  0.6639745  -0.7029829  -0.73584586  0.80450904\n",
            "  -0.5775426   0.07245612  0.2888424   0.5507639 ]]\n",
            "\n",
            "[[0.33762157 0.0739243  0.11595146 0.02955386 0.02859841 0.13344723\n",
            "  0.03350365 0.06417754 0.07968166 0.10354031]]\n",
            "\n",
            "2.014049\n",
            "\n",
            "Epoch 1/5\n",
            "235/235 [==============================] - 2s 4ms/step - loss: 1.6603 - accuracy: 0.6388\n",
            "Epoch 2/5\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.7229 - accuracy: 0.8916\n",
            "Epoch 3/5\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.3964 - accuracy: 0.9264\n",
            "Epoch 4/5\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2767 - accuracy: 0.9421\n",
            "Epoch 5/5\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2132 - accuracy: 0.9539\n",
            "\n",
            "40/40 - 0s - loss: 0.2984 - accuracy: 0.9176 - 218ms/epoch - 5ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.29839134216308594, 0.9175999760627747]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqVsNxQUDPMO"
      },
      "source": [
        "#Appendix B Code Modification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jsjGMvRCDHVZ",
        "outputId": "203f1663-d010-41d2-de86-28ce049adedd"
      },
      "source": [
        "modelB = tf.keras.models.Sequential([ #Layer format for the neural net\n",
        "  tf.keras.layers.Flatten(input_shape=(28, 28)), #Each pixel (grayscale value) mapped to one of 784 nodes\n",
        "  tf.keras.layers.Dense(units=128, activation='relu'), #Fully connected to hidden layer with 'relu'\n",
        "  tf.keras.layers.Dropout(0.30), #Dropout layer randomly sets its input units to 0 at 30% rate at each training step\n",
        "  tf.keras.layers.Dense(units=64, activation='relu'), #Add hidden layer with size=64 'relu'\n",
        "  tf.keras.layers.Dense(units=28, activation='relu'), #Add hidden layer with size=28 'relu'\n",
        "  tf.keras.layers.Dense(units=10) #Final output layer\n",
        "])\n",
        "\n",
        "predictionsB = modelB(X_train[:1]).numpy() #numpy() converts the tensor output\n",
        "print(\"===========untrained output of first training set input===========\") #For each example, model returns a vector of 'logit' scores, one for each class\n",
        "#A tensor where highest value indicates most likely output\n",
        "print(predictionsB) #Print predictions\n",
        "\n",
        "tf.nn.softmax(predictionsB).numpy() #tf.nn.softmax function converts logits to \"probabilities\" for each class\n",
        "print('')\n",
        "print(tf.nn.softmax(predictionsB).numpy()) #Print softmax predictions\n",
        "\n",
        "loss_fnB = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) #Loss equal to negative log probability of the true class\n",
        "#'0' if sure of correct class\n",
        "\n",
        "print('')\n",
        "print(loss_fnB(y_train[:1], predictionsB).numpy()) #Initial untrained model should give loss as -tf.math.log(1/10)~~ 2.3\n",
        "print('')\n",
        "\n",
        "#Put together the NN with training process, loss, and means of evaluation\n",
        "modelB.compile(optimizer='adam',\n",
        "              loss=loss_fnB,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#Model.fit method adjusts the model parameters to minimize the loss\n",
        "modelB.fit(X_train, y_train, epochs=5) #Train it with 5 epochs\n",
        "print('')\n",
        "\n",
        "#Model.evaluate method checks the models performance on a 'Validation-set' or 'Test-set'\n",
        "modelB.evaluate(X_test,  y_test, verbose=2) #Accuracy = fraction of correct test pairs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===========untrained output of first training set input===========\n",
            "[[-0.44597062  0.13044488  0.06301535 -0.18137233  0.38030982  0.4295041\n",
            "   0.11816728  0.1465842   0.00912652  0.06478774]]\n",
            "\n",
            "[[0.05800332 0.10322548 0.09649452 0.07557295 0.13252623 0.1392088\n",
            "  0.10196587 0.10490499 0.09143217 0.0966657 ]]\n",
            "\n",
            "1.9717804\n",
            "\n",
            "Epoch 1/5\n",
            "235/235 [==============================] - 1s 3ms/step - loss: 0.8445 - accuracy: 0.7375\n",
            "Epoch 2/5\n",
            "235/235 [==============================] - 1s 3ms/step - loss: 0.3311 - accuracy: 0.9020\n",
            "Epoch 3/5\n",
            "235/235 [==============================] - 1s 3ms/step - loss: 0.2340 - accuracy: 0.9308\n",
            "Epoch 4/5\n",
            "235/235 [==============================] - 1s 3ms/step - loss: 0.1906 - accuracy: 0.9420\n",
            "Epoch 5/5\n",
            "235/235 [==============================] - 1s 3ms/step - loss: 0.1638 - accuracy: 0.9523\n",
            "\n",
            "40/40 - 0s - loss: 0.2416 - accuracy: 0.9224 - 159ms/epoch - 4ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.24158601462841034, 0.9223999977111816]"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0Mv_mj0Iulw"
      },
      "source": [
        "#Appendix C Code Modification\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osUvT8vrIm9R",
        "outputId": "6b33d068-7dfd-4277-ce94-4ce40c772f45"
      },
      "source": [
        "modelC = tf.keras.models.Sequential([ #Layer format for the neural net\n",
        "  tf.keras.layers.Flatten(input_shape=(28, 28)), #Each pixel (grayscale value) mapped to one of 784 nodes\n",
        "  tf.keras.layers.Dense(units=256, activation='relu'), #Fully connected to hidden layer with 'relu'\n",
        "  tf.keras.layers.Dropout(0.20), #Dropout layer randomly sets its input units to 0 at 20% rate at each training step\n",
        "  tf.keras.layers.LeakyReLU(alpha=0.3), #LeakyReLU layer - negative slope coefficient at 0.3\n",
        "  tf.keras.layers.Dense(units=128, activation='relu'), #Add hidden layer with size=128 'relu\n",
        "  tf.keras.layers.Dense(units=96, activation='relu'), #Add hidden layer with size=96 'relu'\n",
        "  tf.keras.layers.Dense(units=42, activation='relu'), #Add hidden layer with size=42 'relu'\n",
        "  tf.keras.layers.Dense(units=10) #Final output layer\n",
        "])\n",
        "\n",
        "predictionsC = modelC(X_train[:1]).numpy() #numpy() converts the tensor output\n",
        "print(\"===========untrained output of first training set input===========\") #For each example, model returns a vector of 'logit' scores, one for each class\n",
        "#A tensor where highest value indicates most likely output\n",
        "print(predictionsC) #Print predictions\n",
        "\n",
        "tf.nn.softmax(predictionsC).numpy() #tf.nn.softmax function converts logits to \"probabilities\" for each class\n",
        "print('')\n",
        "print(tf.nn.softmax(predictionsC).numpy()) #Print softmax predictions\n",
        "\n",
        "loss_fnC = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) #Loss equal to negative log probability of the true class\n",
        "#'0' if sure of correct class\n",
        "\n",
        "print('')\n",
        "print(loss_fnC(y_train[:1], predictionsC).numpy()) #Initial untrained model should give loss as -tf.math.log(1/10)~~ 2.3\n",
        "print('')\n",
        "\n",
        "#Put together the NN with training process, loss, and means of evaluation\n",
        "modelC.compile(optimizer='adam',\n",
        "              loss=loss_fnC,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#Model.fit method adjusts the model parameters to minimize the loss\n",
        "modelC.fit(X_train, y_train, epochs=5) #Train it with 5 epochs\n",
        "print('')\n",
        "\n",
        "#Model.evaluate method checks the models performance on a 'Validation-set' or 'Test-set'\n",
        "modelC.evaluate(X_test,  y_test, verbose=2) #Accuracy = fraction of correct test pairs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===========untrained output of first training set input===========\n",
            "[[-0.6519744   0.0958652   0.09562487  0.08635166  0.3799377  -0.19937691\n",
            "  -0.39145115  0.00081263  0.27409098  0.02948064]]\n",
            "\n",
            "[[0.05150548 0.1088018  0.10877565 0.10777162 0.14454637 0.08098677\n",
            "  0.06683397 0.09893621 0.1300285  0.10181356]]\n",
            "\n",
            "2.5134695\n",
            "\n",
            "Epoch 1/5\n",
            "235/235 [==============================] - 2s 5ms/step - loss: 0.6509 - accuracy: 0.7911\n",
            "Epoch 2/5\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2311 - accuracy: 0.9319\n",
            "Epoch 3/5\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1638 - accuracy: 0.9507\n",
            "Epoch 4/5\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1109 - accuracy: 0.9668\n",
            "Epoch 5/5\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1003 - accuracy: 0.9688\n",
            "\n",
            "40/40 - 0s - loss: 0.2324 - accuracy: 0.9288 - 247ms/epoch - 6ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.2324375957250595, 0.9287999868392944]"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfsgxDcwNnUL"
      },
      "source": [
        "#Appendix D Code Modification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYyeLb4yVjV3",
        "outputId": "3c963dde-5456-43d3-e6d0-1e83028f2b44"
      },
      "source": [
        "mnist=tf.keras.datasets.mnist #One of a handful of data sets known to Keras/TensorFlow\n",
        "#mnist.load_data() produces a pair of inpu/output tensors for training\n",
        "#and one for testing\n",
        "\n",
        "#Reduce \n",
        "(X_train,y_train), (X_test,y_test) = mnist.load_data()\n",
        "\n",
        "X_train,X_test=X_train/255, X_test/255  #Scale down input\n",
        "\n",
        "modelD = tf.keras.models.Sequential([ #Layer format for the neural net\n",
        "  tf.keras.layers.Flatten(input_shape=(28, 28)), #Each pixel (grayscale value) mapped to one of 784 nodes\n",
        "  tf.keras.layers.Dense(units=256, activation='relu'), #Fully connected to hidden layer with 'relu'\n",
        "  tf.keras.layers.Dropout(0.20), #Dropout layer randomly sets its input units to 0 at 20% rate at each training step\n",
        "  tf.keras.layers.LeakyReLU(alpha=0.3), #LeakyReLU layer - negative slope coefficient at 0.3\n",
        "  tf.keras.layers.Dropout(0.30), #Dropout layer randomly sets units to 0 at 30% rate at each training step\n",
        "  tf.keras.layers.Dense(units=96, activation='relu'), #Add hidden layer with size=96 'relu'\n",
        "  tf.keras.layers.Dropout(0.30), #Dropout layer randomly sets units to 0 at 30% rate at each training step\n",
        "  tf.keras.layers.Dense(units=42, activation='relu'), #Add hidden layer with size=42 'relu'\n",
        "  tf.keras.layers.Dropout(0.30), #Dropout layer randomly sets units to 0 at 30% rate at each training step\n",
        "  tf.keras.layers.Dense(units=10) #Final output layer\n",
        "])\n",
        "\n",
        "predictionsD = modelD(X_train[:1]).numpy() #numpy() converts the tensor output\n",
        "print(\"===========untrained output of first training set input===========\") #For each example, model returns a vector of 'logit' scores, one for each class\n",
        "#A tensor where highest value indicates most likely output\n",
        "print(predictionsD) #Print predictions\n",
        "\n",
        "tf.nn.softmax(predictionsD).numpy() #tf.nn.softmax function converts logits to \"probabilities\" for each class\n",
        "print('')\n",
        "print(tf.nn.softmax(predictionsD).numpy()) #Print softmax predictions\n",
        "\n",
        "loss_fnD = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) #Loss equal to negative log probability of the true class\n",
        "#'0' if sure of correct class\n",
        "\n",
        "print('')\n",
        "print(loss_fnD(y_train[:1], predictionsD).numpy()) #Initial untrained model should give loss as -tf.math.log(1/10)~~ 2.3\n",
        "print('')\n",
        "\n",
        "#Put together the NN with training process, loss, and means of evaluation\n",
        "modelD.compile(optimizer='adam',\n",
        "              loss=loss_fnD,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#Model.fit method adjusts the model parameters to minimize the loss\n",
        "modelD.fit(X_train, y_train, epochs=40) #Train it with 5 epochs\n",
        "print('')\n",
        "\n",
        "#Model.evaluate method checks the models performance on a 'Validation-set' or 'Test-set'\n",
        "modelD.evaluate(X_test,  y_test, verbose=2) #Accuracy = fraction of correct test pairs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===========untrained output of first training set input===========\n",
            "[[ 0.39843574  0.05194907 -0.1503809   0.17238015 -0.1955871  -0.23457094\n",
            "  -0.10793986  0.01675963 -0.19781418 -0.4744837 ]]\n",
            "\n",
            "[[0.15577377 0.11015826 0.08998007 0.12425664 0.08600298 0.08271476\n",
            "  0.09388111 0.10634927 0.08581166 0.06507142]]\n",
            "\n",
            "2.4923573\n",
            "\n",
            "Epoch 1/40\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.4782 - accuracy: 0.8602\n",
            "Epoch 2/40\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2378 - accuracy: 0.9363\n",
            "Epoch 3/40\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1916 - accuracy: 0.9488\n",
            "Epoch 4/40\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1677 - accuracy: 0.9548\n",
            "Epoch 5/40\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1489 - accuracy: 0.9592\n",
            "Epoch 6/40\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1419 - accuracy: 0.9623\n",
            "Epoch 7/40\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1281 - accuracy: 0.9648\n",
            "Epoch 8/40\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1208 - accuracy: 0.9672\n",
            "Epoch 9/40\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1134 - accuracy: 0.9686\n",
            "Epoch 10/40\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1063 - accuracy: 0.9711\n",
            "Epoch 11/40\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1048 - accuracy: 0.9703\n",
            "Epoch 12/40\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0990 - accuracy: 0.9720\n",
            "Epoch 13/40\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0973 - accuracy: 0.9731\n",
            "Epoch 14/40\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0892 - accuracy: 0.9750\n",
            "Epoch 15/40\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0903 - accuracy: 0.9747\n",
            "Epoch 16/40\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0869 - accuracy: 0.9758\n",
            "Epoch 17/40\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0845 - accuracy: 0.9770\n",
            "Epoch 18/40\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0790 - accuracy: 0.9774\n",
            "Epoch 19/40\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0789 - accuracy: 0.9781\n",
            "Epoch 20/40\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0757 - accuracy: 0.9789\n",
            "Epoch 21/40\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0758 - accuracy: 0.9781\n",
            "Epoch 22/40\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0736 - accuracy: 0.9789\n",
            "Epoch 23/40\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0705 - accuracy: 0.9806\n",
            "Epoch 24/40\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0717 - accuracy: 0.9801\n",
            "Epoch 25/40\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0677 - accuracy: 0.9803\n",
            "Epoch 26/40\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0663 - accuracy: 0.9812\n",
            "Epoch 27/40\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0674 - accuracy: 0.9807\n",
            "Epoch 28/40\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0658 - accuracy: 0.9816\n",
            "Epoch 29/40\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0648 - accuracy: 0.9819\n",
            "Epoch 30/40\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0625 - accuracy: 0.9829\n",
            "Epoch 31/40\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0623 - accuracy: 0.9826\n",
            "Epoch 32/40\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0596 - accuracy: 0.9833\n",
            "Epoch 33/40\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0596 - accuracy: 0.9829\n",
            "Epoch 34/40\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0583 - accuracy: 0.9835\n",
            "Epoch 35/40\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0581 - accuracy: 0.9842\n",
            "Epoch 36/40\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0583 - accuracy: 0.9838\n",
            "Epoch 37/40\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0573 - accuracy: 0.9840\n",
            "Epoch 38/40\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0564 - accuracy: 0.9842\n",
            "Epoch 39/40\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0568 - accuracy: 0.9839\n",
            "Epoch 40/40\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0513 - accuracy: 0.9856\n",
            "\n",
            "313/313 - 1s - loss: 0.0836 - accuracy: 0.9819 - 603ms/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.0836314857006073, 0.9818999767303467]"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    }
  ]
}